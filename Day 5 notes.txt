------Day 5

AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name prod \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "prod" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=prod'
[ℹ]  2 sequential tasks: { create cluster control plane "prod", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-prod-cluster"
[ℹ]  deploying stack "eksctl-prod-cluster"
^C
AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name prod \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "prod" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=prod'
[ℹ]  2 sequential tasks: { create cluster control plane "prod", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-prod-cluster"
[ℹ]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
[ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-1 --name=prod'
[✖]  creating CloudFormation stack "eksctl-prod-cluster": AlreadyExistsException: Stack [eksctl-prod-cluster] already exists
        status code: 400, request id: aa24f47a-b4f0-11e9-aa01-7d729f748561
[✖]  failed to create cluster "prod"
AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name prod \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "prod" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=prod'
[ℹ]  2 sequential tasks: { create cluster control plane "prod", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-prod-cluster"
[ℹ]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
[ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-1 --name=prod'
[✖]  creating CloudFormation stack "eksctl-prod-cluster": AlreadyExistsException: Stack [eksctl-prod-cluster] already exists
        status code: 400, request id: e489882c-b4f0-11e9-9790-f5568e636f32
[✖]  failed to create cluster "prod"
AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name prod_new \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto\
> --appmesh-access
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[✖]  unable to find image "auto--appmesh-access": InvalidAMIID.Malformed: Invalid id: "auto--appmesh-access" (expecting "ami-...")
        status code: 400, request id: eee2f967-206f-44e2-909e-8048cd2e6963
AP-ROLE-NAME6:~/environment $  eksctl create cluster \
> --region us-east-1 \
> --name prod_new \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto \
> --appmesh-access
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "prod_new" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=prod_new'
[ℹ]  2 sequential tasks: { create cluster control plane "prod_new", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-prod_new-cluster"
[ℹ]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
[ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-1 --name=prod_new'
[✖]  creating CloudFormation stack "eksctl-prod_new-cluster": ValidationError: 1 validation error detected: Value 'eksctl-prod_new-cluster' at 'stackName' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z][-a-zA-Z0-9]*
        status code: 400, request id: 4e336630-b4f1-11e9-b0db-0fad991e3271
[✖]  failed to create cluster "prod_new"
AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name new_prod \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto \
> --appmesh-access
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "new_prod" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=new_prod'
[ℹ]  2 sequential tasks: { create cluster control plane "new_prod", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-new_prod-cluster"
[ℹ]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
[ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-1 --name=new_prod'
[✖]  creating CloudFormation stack "eksctl-new_prod-cluster": ValidationError: 1 validation error detected: Value 'eksctl-new_prod-cluster' at 'stackName' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z][-a-zA-Z0-9]*
        status code: 400, request id: 5b28341f-b4f1-11e9-aef8-c52ef64212fd
[✖]  failed to create cluster "new_prod"
AP-ROLE-NAME6:~/environment $ eksctl create cluster \
> --region us-east-1 \
> --name newprod \
> --version 1.13 \
> --nodegroup-name standard-workers \
> --node-type t3.medium \
> --zones=us-east-1a,us-east-1b,us-east-1c \
> --nodes 4 \
> --nodes-min 4 \
> --nodes-max 4 \
> --node-ami auto \
> --appmesh-access
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "newprod" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=newprod'
[ℹ]  2 sequential tasks: { create cluster control plane "newprod", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-newprod-cluster"
[ℹ]  deploying stack "eksctl-newprod-cluster"
eksctl create cluster \
--region us-east-1 \
--name newprod \
--version 1.13 \
--nodegroup-name standard-workers \
--node-type t3.medium \
--zones=us-east-1a,us-east-1b,us-east-1c \
--nodes 4 \
--nodes-min 4 \
--nodes-max 4 \
--node-ami auto \
--appmesh-access
  


eksctl create cluster \
--region us-east-1 \
--name newprod \
--version 1.13 \
--nodegroup-name standard-workers \
--node-type t3.medium \
--zones=us-east-1a,us-east-1b,us-east-1c \
--nodes 4 \
--nodes-min 4 \
--nodes-max 4 \
--node-ami auto \
--appmesh-access
^C
AP-ROLE-NAME6:~/environment $ eksctl create cluster --region us-east-1 --name prodnew --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto --appmesh-access                                                                                                                            
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  nodegroup "standard-workers" will use "ami-0f2e8e5663e16b436" [AmazonLinux2/1.13]
[ℹ]  using Kubernetes version 1.13
[ℹ]  creating EKS cluster "prodnew" in "us-east-1" region
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=prodnew'
[ℹ]  2 sequential tasks: { create cluster control plane "prodnew", create nodegroup "standard-workers" }
[ℹ]  building cluster stack "eksctl-prodnew-cluster"
[ℹ]  deploying stack "eksctl-prodnew-cluster"
[ℹ]  building nodegroup stack "eksctl-prodnew-nodegroup-standard-workers"
[ℹ]  deploying stack "eksctl-prodnew-nodegroup-standard-workers"
[✔]  all EKS cluster resource for "prodnew" had been created
[✔]  saved kubeconfig as "/home/ec2-user/.kube/config"
[ℹ]  adding role "arn:aws:iam::960252834999:role/eksctl-prodnew-nodegroup-standard-NodeInstanceRole-1CZQ84212H2YR" to auth ConfigMap
[ℹ]  nodegroup "standard-workers" has 0 node(s)
[ℹ]  waiting for at least 4 node(s) to become ready in "standard-workers"
[ℹ]  nodegroup "standard-workers" has 4 node(s)
[ℹ]  node "ip-192-168-15-204.ec2.internal" is ready
[ℹ]  node "ip-192-168-27-227.ec2.internal" is ready
[ℹ]  node "ip-192-168-46-182.ec2.internal" is ready
[ℹ]  node "ip-192-168-74-229.ec2.internal" is ready
[ℹ]  kubectl command should work with "/home/ec2-user/.kube/config", try 'kubectl get nodes'
[✔]  EKS cluster "prodnew" in "us-east-1" region is ready
AP-ROLE-NAME6:~/environment $ 

AP-ROLE-NAME6:~/environment $ kubectl get nodes
NAME                             STATUS   ROLES    AGE   VERSION
ip-192-168-15-204.ec2.internal   Ready    <none>   92s   v1.13.7-eks-c57ff8
ip-192-168-27-227.ec2.internal   Ready    <none>   91s   v1.13.7-eks-c57ff8
ip-192-168-46-182.ec2.internal   Ready    <none>   93s   v1.13.7-eks-c57ff8
ip-192-168-74-229.ec2.internal   Ready    <none>   92s   v1.13.7-eks-c57ff8
AP-ROLE-NAME6:~/environment $ 

-------------applicaion running ----

AP-ROLE-NAME6:~/environment $ git clone https://github.com/aws/aws-app-mesh-examples
Cloning into 'aws-app-mesh-examples'...
remote: Enumerating objects: 25, done.
remote: Counting objects: 100% (25/25), done.
remote: Compressing objects: 100% (23/23), done.
remote: Total 1575 (delta 2), reused 24 (delta 2), pack-reused 1550
Receiving objects: 100% (1575/1575), 5.21 MiB | 36.02 MiB/s, done.
Resolving deltas: 100% (648/648), done.
AP-ROLE-NAME6:~/environment $ cd aws-app-mesh-examples/examples/apps/djapp/
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ ll
total 48
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 1_create_the_initial_architecture
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 2_create_injector
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 3_add_crds
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 4_create_initial_mesh_components
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 5_canary
-rwxrwxr-x 1 ec2-user ec2-user 2181 Aug  2 06:52 amctl.sh
-rw-rw-r-- 1 ec2-user ec2-user  324 Aug  2 06:52 awscli.yaml
-rwxrwxr-x 1 ec2-user ec2-user  950 Aug  2 06:52 cleanup.sh
-rwxrwxr-x 1 ec2-user ec2-user 2253 Aug  2 06:52 create.sh
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 img
-rwxrwxr-x 1 ec2-user ec2-user  312 Aug  2 06:52 ranCon.sh
-rw-rw-r-- 1 ec2-user ec2-user 2149 Aug  2 06:52 README.md
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ cd ..
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps (master) $ ll
total 12
drwxrwxr-x 7 ec2-user ec2-user 4096 Aug  2 06:52 colorapp
drwxrwxr-x 8 ec2-user ec2-user 4096 Aug  2 06:52 djapp
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 voteapp
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps (master) $ cd aws-app-mesh-examples/examples/apps/djapp/
bash: cd: aws-app-mesh-examples/examples/apps/djapp/: No such file or directory
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps (master) $ cd ..
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples (master) $ ll
total 12
drwxrwxr-x 5 ec2-user ec2-user 4096 Aug  2 06:52 apps
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 infrastructure
-rw-rw-r-- 1 ec2-user ec2-user 2197 Aug  2 06:52 README.md
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples (master) $ cd apps
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps (master) $ cd djapp
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ ll
total 48
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 1_create_the_initial_architecture
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 2_create_injector
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 3_add_crds
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 4_create_initial_mesh_components
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 5_canary
-rwxrwxr-x 1 ec2-user ec2-user 2181 Aug  2 06:52 amctl.sh
-rw-rw-r-- 1 ec2-user ec2-user  324 Aug  2 06:52 awscli.yaml
-rwxrwxr-x 1 ec2-user ec2-user  950 Aug  2 06:52 cleanup.sh
-rwxrwxr-x 1 ec2-user ec2-user 2253 Aug  2 06:52 create.sh
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 img
-rwxrwxr-x 1 ec2-user ec2-user  312 Aug  2 06:52 ranCon.sh
-rw-rw-r-- 1 ec2-user ec2-user 2149 Aug  2 06:52 README.md
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ echo $ROLE_NAME

AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ echo $AWS_REGION

AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ echo "export ACCOUNT_ID=${ACCOUNT_ID}" >> ~/.bash_profile
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ echo "export AWS_REGION=${AWS_REGION}" >> ~/.bash_profile
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ aws configure set default.region ${AWS_REGION}
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ aws configure get default.region
us-east-1
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ aws iam get-role-policy --role-name $ROLE_NAME --policy-name AM-Policy-For-Worker
usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:

  aws help
  aws <command> help
  aws <command> <subcommand> help
aws: error: argument --role-name: expected one argument
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ ll
total 48
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 1_create_the_initial_architecture
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 2_create_injector
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 3_add_crds
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 4_create_initial_mesh_components
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 5_canary
-rwxrwxr-x 1 ec2-user ec2-user 2181 Aug  2 06:52 amctl.sh
-rw-rw-r-- 1 ec2-user ec2-user  324 Aug  2 06:52 awscli.yaml
-rwxrwxr-x 1 ec2-user ec2-user  950 Aug  2 06:52 cleanup.sh
-rwxrwxr-x 1 ec2-user ec2-user 2253 Aug  2 06:52 create.sh
drwxrwxr-x 2 ec2-user ec2-user 4096 Aug  2 06:52 img
-rwxrwxr-x 1 ec2-user ec2-user  312 Aug  2 06:52 ranCon.sh
-rw-rw-r-- 1 ec2-user ec2-user 2149 Aug  2 06:52 README.md
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -f 1_create_the_initial_architecture/1_prod_ns.yaml
namespace/prod created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_deployment.yaml
deployment.apps/dj created
deployment.apps/metal-v1 created
deployment.apps/jazz-v1 created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_services.yaml
service/dj created
service/metal-v1 created
service/jazz-v1 created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get all -nprod
NAME                            READY   STATUS    RESTARTS   AGE
pod/dj-7fbb4d499c-gk5dn         1/1     Running   0          45s
pod/jazz-v1-6d5dd497f-vrhxz     1/1     Running   0          45s
pod/metal-v1-746d77c67b-55t8j   1/1     Running   0          45s


NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/dj         ClusterIP   10.100.56.206    <none>        9080/TCP   19s
service/jazz-v1    ClusterIP   10.100.131.109   <none>        9080/TCP   19s
service/metal-v1   ClusterIP   10.100.145.1     <none>        9080/TCP   19s


NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dj         1/1     1            1           45s
deployment.apps/jazz-v1    1/1     1            1           45s
deployment.apps/metal-v1   1/1     1            1           45s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/dj-7fbb4d499c         1         1         1       45s
replicaset.apps/jazz-v1-6d5dd497f     1         1         1       45s
replicaset.apps/metal-v1-746d77c67b   1         1         1       45s

AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nprod -l app=dj
NAME                  READY   STATUS    RESTARTS   AGE
dj-7fbb4d499c-gk5dn   1/1     Running   0          2m24s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl exec -nprod -it dj-7fbb4d499c-gk5dn bash
root@dj-7fbb4d499c-gk5dn:/usr/src/app# ll
bash: ll: command not found
root@dj-7fbb4d499c-gk5dn:/usr/src/app# ls
Dockerfile  image_push.sh  node_modules  package-lock.json  package.json  server.js
root@dj-7fbb4d499c-gk5dn:/usr/src/app# curl jazz-v1.prod.svc.cluster.local:9080;echo
["Astrud Gilberto","Miles Davis"]
root@dj-7fbb4d499c-gk5dn:/usr/src/app# curl metal-v1.prod.svc.cluster.local:9080;echo
["Megadeth","Judas Priest"]
root@dj-7fbb4d499c-gk5dn:/usr/src/app# exit
exit
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ cd 2_create_injector
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector (master) $ ll
total 32
-rw-rw-r-- 1 ec2-user ec2-user   68 Aug  2 06:52 appmesh-ns.yaml
-rwxrwxr-x 1 ec2-user ec2-user  592 Aug  2 06:52 ca-bundle.sh
-rwxrwxr-x 1 ec2-user ec2-user  279 Aug  2 06:52 create.sh
-rwxrwxr-x 1 ec2-user ec2-user  427 Aug  2 06:52 delete.sh
-rwxrwxr-x 1 ec2-user ec2-user 2196 Aug  2 06:52 gen-cert.sh
-rw-rw-r-- 1 ec2-user ec2-user 4365 Aug  2 06:52 inject.yaml
-rw-rw-r-- 1 ec2-user ec2-user 2964 Aug  2 06:52 inject.yaml.template
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector (master) $ ./create.sh
namespace/appmesh-inject created
creating certs in tmpdir /tmp/tmp.8LgSKNgDN7 
Generating RSA private key, 2048 bit long modulus
..........................................................................................................................................................+++
..........................+++
e is 65537 (0x10001)
certificatesigningrequest.certificates.k8s.io/aws-app-mesh-inject.appmesh-inject created
NAME                                 AGE   REQUESTOR          CONDITION
aws-app-mesh-inject.appmesh-inject   0s    kubernetes-admin   Pending
certificatesigningrequest.certificates.k8s.io/aws-app-mesh-inject.appmesh-inject approved
secret/aws-app-mesh-inject created

processing templates
Created injector manifest at:/home/ec2-user/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector/inject.yaml

serviceaccount/aws-app-mesh-inject-sa created
clusterrole.rbac.authorization.k8s.io/aws-app-mesh-inject-cr created
clusterrolebinding.rbac.authorization.k8s.io/aws-app-mesh-inject-binding created
service/aws-app-mesh-inject created
deployment.apps/aws-app-mesh-inject created
mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-app-mesh-inject created

Waiting for pods to come up...

App Inject Pods and Services After Install:

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
aws-app-mesh-inject   ClusterIP   10.100.250.27   <none>        443/TCP   15s
NAME                                  READY   STATUS    RESTARTS   AGE
aws-app-mesh-inject-fcc965769-5z2mh   1/1     Running   0          15s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector (master) $ 
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector (master) $ error
bash: error: command not found
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp/2_create_injector (master) $  cd ..
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl label namespace prod appmesh.k8s.aws/sidecarInjectorWebhook=enabled
namespace/prod labeled
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nappmesh-inject
NAME                                  READY   STATUS    RESTARTS   AGE
aws-app-mesh-inject-fcc965769-5z2mh   1/1     Running   0          3m31s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nprod
NAME                        READY   STATUS    RESTARTS   AGE
dj-7fbb4d499c-gk5dn         1/1     Running   0          10m
jazz-v1-6d5dd497f-vrhxz     1/1     Running   0          10m
metal-v1-746d77c67b-55t8j   1/1     Running   0          10m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nappmesh-inject
NAME                                  READY   STATUS    RESTARTS   AGE
aws-app-mesh-inject-fcc965769-5z2mh   1/1     Running   0          3m50s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -f 3_add_crds/mesh-definition.yaml
customresourcedefinition.apiextensions.k8s.io/meshes.appmesh.k8s.aws created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -f 3_add_crds/virtual-node-definition.yaml
customresourcedefinition.apiextensions.k8s.io/virtualnodes.appmesh.k8s.aws created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -f 3_add_crds/virtual-service-definition.yaml
customresourcedefinition.apiextensions.k8s.io/virtualservices.appmesh.k8s.aws created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -f 3_add_crds/controller-deployment.yaml
namespace/appmesh-system created
deployment.apps/app-mesh-controller created
serviceaccount/app-mesh-sa created
clusterrole.rbac.authorization.k8s.io/app-mesh-controller created
clusterrolebinding.rbac.authorization.k8s.io/app-mesh-controller-binding created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nappmesh-system
NAME                                   READY   STATUS    RESTARTS   AGE
app-mesh-controller-5bb6cf64cd-sz4c5   1/1     Running   0          16s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ 

AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl create -f 4_create_initial_mesh_components/mesh.yaml
mesh.appmesh.k8s.aws/dj-app created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get meshes -nprod
NAME     AGE
dj-app   16s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ aws appmesh list-meshes
{
    "meshes": [
        {
            "meshName": "dj-app", 
            "arn": "arn:aws:appmesh:us-east-1:960252834999:mesh/dj-app"
        }
    ]
}
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ aws appmesh describe-mesh --mesh-name dj-app
{
    "mesh": {
        "status": {
            "status": "ACTIVE"
        }, 
        "meshName": "dj-app", 
        "spec": {}, 
        "metadata": {
            "version": 1, 
            "lastUpdatedAt": 1564731092.827, 
            "createdAt": 1564731092.827, 
            "arn": "arn:aws:appmesh:us-east-1:960252834999:mesh/dj-app", 
            "uid": "b832de05-bb2d-4a78-94c7-b7802aad2fb4"
        }
    }
}
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl create -f 4_create_initial_mesh_components/nodes_representing_virtual_services.yaml
virtualnode.appmesh.k8s.aws/metal created
virtualnode.appmesh.k8s.aws/jazz created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl create -nprod -f 4_create_initial_mesh_components/nodes_representing_physical_services.yaml
virtualnode.appmesh.k8s.aws/dj created
virtualnode.appmesh.k8s.aws/jazz-v1 created
virtualnode.appmesh.k8s.aws/metal-v1 created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get virtualnodes -nprod
NAME       AGE
dj         39s
jazz       1m
jazz-v1    39s
metal      1m
metal-v1   39s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl apply -nprod -f 4_create_initial_mesh_components/virtual-services.yaml
virtualservice.appmesh.k8s.aws/jazz.prod.svc.cluster.local created
virtualservice.appmesh.k8s.aws/metal.prod.svc.cluster.local created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get servcie
error: the server doesn't have a resource type "servcie"
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   41m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   42m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get svc -nprod
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
dj         ClusterIP   10.100.56.206    <none>        9080/TCP   28m
jazz-v1    ClusterIP   10.100.131.109   <none>        9080/TCP   28m
metal-v1   ClusterIP   10.100.145.1     <none>        9080/TCP   28m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl create -nprod -f 4_create_initial_mesh_components/metal_and_jazz_placeholder_services.yaml
service/jazz created
service/metal created
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get -nprod virtualservices
NAME                           AGE
jazz.prod.svc.cluster.local    4m
metal.prod.svc.cluster.local   4m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get svc -nprod
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
dj         ClusterIP   10.100.56.206    <none>        9080/TCP   29m
jazz       ClusterIP   10.100.160.245   <none>        9080/TCP   27s
jazz-v1    ClusterIP   10.100.131.109   <none>        9080/TCP   29m
metal      ClusterIP   10.100.204.149   <none>        9080/TCP   27s
metal-v1   ClusterIP   10.100.145.1     <none>        9080/TCP   29m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nprod
NAME                        READY   STATUS    RESTARTS   AGE
dj-7fbb4d499c-gk5dn         1/1     Running   0          30m
jazz-v1-6d5dd497f-vrhxz     1/1     Running   0          30m
metal-v1-746d77c67b-55t8j   1/1     Running   0          30m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl describe pods/dj-5b445fbdf4-qf8sv -nprod
Error from server (NotFound): pods "dj-5b445fbdf4-qf8sv" not found
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl describe pods/dj-7fbb4d499c-gk5dn -nprod
Name:           dj-7fbb4d499c-gk5dn
Namespace:      prod
Priority:       0
Node:           ip-192-168-15-204.ec2.internal/192.168.15.204
Start Time:     Fri, 02 Aug 2019 07:12:43 +0000
Labels:         app=dj
                pod-template-hash=7fbb4d499c
                version=v1
Annotations:    kubernetes.io/psp: eks.privileged
Status:         Running
IP:             192.168.6.253
Controlled By:  ReplicaSet/dj-7fbb4d499c
Containers:
  dj:
    Container ID:   docker://81d1f83069161b7694cd3794f775297ebe91194465b4504c9a264fed817e6a4b
    Image:          672518094988.dkr.ecr.us-west-2.amazonaws.com/hello-world:v1.0
    Image ID:       docker-pullable://672518094988.dkr.ecr.us-west-2.amazonaws.com/hello-world@sha256:52757babfe481adc8419bcc96b5e7f3da9e586b22006a985d9cc3650916ff31f
    Port:           9080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 02 Aug 2019 07:13:06 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      HW_RESPONSE:  DJ Reporting for duty!
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4sh52 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-4sh52:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-4sh52
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                                     Message
  ----    ------     ----  ----                                     -------
  Normal  Scheduled  31m   default-scheduler                        Successfully assigned prod/dj-7fbb4d499c-gk5dn to ip-192-168-15-204.ec2.internal
  Normal  Pulling    31m   kubelet, ip-192-168-15-204.ec2.internal  pulling image "672518094988.dkr.ecr.us-west-2.amazonaws.com/hello-world:v1.0"
  Normal  Pulled     31m   kubelet, ip-192-168-15-204.ec2.internal  Successfully pulled image "672518094988.dkr.ecr.us-west-2.amazonaws.com/hello-world:v1.0"
  Normal  Created    31m   kubelet, ip-192-168-15-204.ec2.internal  Created container
  Normal  Started    31m   kubelet, ip-192-168-15-204.ec2.internal  Started container
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nprod
NAME                        READY   STATUS    RESTARTS   AGE
dj-7fbb4d499c-gk5dn         1/1     Running   0          32m
jazz-v1-6d5dd497f-vrhxz     1/1     Running   0          32m
metal-v1-746d77c67b-55t8j   1/1     Running   0          32m
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl patch deployment dj -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
deployment.extensions/dj patched
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl patch deployment metal-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
deployment.extensions/metal-v1 patched
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl patch deployment jazz-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
deployment.extensions/jazz-v1 patched
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ 
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ kubectl get pods -nprod
NAME                        READY   STATUS            RESTARTS   AGE
dj-7fbb4d499c-gk5dn         1/1     Running           0          32m
dj-b59c96cc5-zqwgc          0/2     PodInitializing   0          16s
jazz-v1-5846476449-zng4v    0/2     PodInitializing   0          15s
jazz-v1-6d5dd497f-vrhxz     1/1     Running           0          32m
metal-v1-746d77c67b-55t8j   1/1     Running           0          32m
metal-v1-97ccc8cd-svf6t     0/2     PodInitializing   0          16s
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ 
AP-ROLE-NAME6:~/environment/aws-app-mesh-examples/examples/apps/djapp (master) $ 

----------------------till lunch -----------

sql part


AP-ROLE-NAME6:~/environment $ kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
configmap/mysql created
AP-ROLE-NAME6:~/environment $ kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
service/mysql created
service/mysql-read created
AP-ROLE-NAME6:~/environment $ kubectl apply -f testapp/sql/mysql-statefulset.yaml
statefulset.apps/mysql created
AP-ROLE-NAME6:~/environment $ kubectl getpods
Error: unknown command "getpods" for "kubectl"
Run 'kubectl --help' for usage.
unknown command "getpods" for "kubectl"
AP-ROLE-NAME6:~/environment $ kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
mysql-0   0/2     Init:0/2   0          18s
AP-ROLE-NAME6:~/environment $ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   1/2     Running   0          41s
AP-ROLE-NAME6:~/environment $ kubectl get pods -l app=mysql --watch
NAME      READY   STATUS    RESTARTS   AGE
mysql-0   2/2     Running   0          56s
mysql-1   0/2     Pending   0          8s
^[[A^[[ANAME      READY   STATUS    RESTARTS   AGE
mysql-1   0/2     Pending   0          14s
mysql-1   0/2     Pending   0          15s
mysql-1   0/2     Pending   0          15s
mysql-1   0/2     Init:0/2   0          15s
mysql-1   0/2     Init:1/2   0          43s
mysql-1   0/2     Init:1/2   0          49s
mysql-1   0/2     PodInitializing   0          55s
mysql-1   1/2     Running           0          56s
mysql-1   2/2     Running           0          61s
mysql-2   0/2     Pending           0          0s
mysql-2   0/2     Pending           0          0s
mysql-2   0/2     Pending           0          0s
mysql-2   0/2     Init:0/2          0          0s
mysql-2   0/2     Init:1/2          0          27s
mysql-2   0/2     Init:1/2          0          34s
mysql-2   0/2     PodInitializing   0          40s
mysql-2   1/2     Running           0          41s
mysql-2   2/2     Running           0          46s



AP-ROLE-NAME6:~/environment/testapp (master) $ kubectl get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-mysql-0   Bound    pvc-2e1e8523-b508-11e9-a0cb-125b23201c78   10Gi       RWO            gp2            4m1s
data-mysql-1   Bound    pvc-4a754006-b508-11e9-a0cb-125b23201c78   10Gi       RWO            gp2            3m13s
data-mysql-2   Bound    pvc-6e96adbb-b508-11e9-a0cb-125b23201c78   10Gi       RWO            gp2            2m12s
AP-ROLE-NAME6:~/environment/testapp (master) $ 

CAP-ROLE-NAME6:~/environment $ kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\
>   mysql -h mysql-0.mysql <<EOF
> CREATE DATABASE test;
> CREATE TABLE test.messages (message VARCHAR(250));
> INSERT INTO test.messages VALUES ('hello');
> EOF

If you don't see a command prompt, try pressing enter.
pod "mysql-client" deleted
AP-ROLE-NAME6:~/environment $ 
AP-ROLE-NAME6:~/environment $ kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
>   mysql -h mysql-read -e "SELECT * FROM test.messages"
+---------+
| message |
+---------+
| hello   |
+---------+

pod "mysql-client" deleted
AP-ROLE-NAME6:~/environment $ 
AP-ROLE-NAME6:~/environment $ kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
>   bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"

If you don't see a command prompt, try pressing enter.

+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:30 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2019-08-02 09:39:31 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2019-08-02 09:39:32 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:33 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2019-08-02 09:39:34 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:35 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:36 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2019-08-02 09:39:37 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2019-08-02 09:39:38 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2019-08-02 09:39:39 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:40 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2019-08-02 09:39:41 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2019-08-02 09:39:42 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2019-08-02 09:39:43 |
+-------------+---------------------+
^C
pod "mysql-client-loop" deleted
pod default/mysql-client-loop terminated (Error)
AP-ROLE-NAME6:~/environment $ 



-------------------------------------------------------------------

AP-ROLE-NAME6:~/environment/testapp (master) $ history
    1  pwd
    2  ls
    3  sudo yum -y install docker
    4  docker info
    5  whoami
    6  sudo usermod -a -G docker ec2-user
    7  id -a
    8  sudo service docker start
    9  sudo chkconfig docker on
   10  history
   11  sudo service docker status
   12  docker info
   13  ll
   14  docker pull http:2.4
   15  docker pull httpd:2.4
   16  docker images
   17  docker inspect httpd
   18  docker inspect httpd:2.4
   19  history
   20  docker run -d --name testapache httpd2.4
   21  docker run -d --name testapache httpd:2.4
   22  docker ps -a
   23  docker stop testapache
   24  docker p
   25  docker ps
   26  docker ps -a
   27  git clone https://github.com/rsubramanian4/testapp.git
   28  docker build -t knowdocker/myapache:1.0 testapp/frontend/
   29  docker images
   30  docker inspect e32ae41a4574
   31  history
   32  docker ps
   33  docke images
   34  dockr images
   35  docker images
   36  docker run -d --name myapache knowdocker/myapache:1.0
   37  docker ps
   38  docker logs myapache
   39  docker stop myapache
   40  docker run -d --name myapache -p 9000:80  knowdocker/myapache:1.0
   41  docker run -d --name myapache9 -p 9000:80  knowdocker/myapache:1.0
   42  curl http://localhost
   43  curl http://localhost:9000
   44  history
   45  curl http://localhost:9000
   46  docker conatiner -rm myapache
   47  docker stop myapache
   48  docker rm myapache
   49  docker stop myapache9
   50  docker rm myapache9
   51  docker run -d --name myapache -p 9000:80  knowdocker/myapache:1.0
   52  curl http://localhost:9000
   53  docker push knowdocker/myapache:1.0
   54  ll
   55  git commit 
   56  git commit https://github.com/rsubramanian4/testapp.git
   57  cd testapp/
   58  git commit https://github.com/rsubramanian4/testapp.git
   59  ll
   60  cd ..
   61  ll
   62  docker login
   63  docker login -u "knowdocker" -p "Dock$2474"
   64  docker login -u knowdocker -p Dock$2474
   65  docker login -uknowdocker -pDock$2474
   66  docker login -u knowdocker -p Dock$2474
   67  https://github.com/rsubramanian4/testapp.git
   68  LL
   69  ll
   70  cd testapp/
   71  ll
   72  git commit
   73  git push
   74  git stage
   75  exit
   76  pwd
   77  ll
   78  exit
   79  docker ps
   80  docker ps -
   81  docker ps -a
   82  docker run -d --name myapache -p 9000:80  knowdocker/myapache:1.0
   83  docker start myapache
   84  docker ps
   85  docket login
   86  docker login
   87  docker push myapache:1.0
   88  docker push knowdocker/myapache:1.0
   89  cd testapp/
   90  ll
   91  git add *
   92  git commit -m "first commit"
   93  git config --global
   94  git config
   95  git config --global
   96  git config
   97  git config --global --list
   98  git init
   99  vi .git
  100  git commit -m "first commit"
  101  git push
  102  history
  103  git commit -m "test commmit"
  104  git add *
  105  git commit -m "test commmit"
  106  git push
  107  git config --global user.name "rsubramanian4"
  108  git config --global user.email mailtoraja.s@gmail.com
  109  git config --list
  110  git add
  111  git add *
  112  git commit -m "second commmit"
  113  git push
  114  history
  115  git add *
  116  git commit -m "day1 commands commmit"
  117  git push
  118  exit
  119  pwd
  120  cd testapp/
  121  git add
  122  git add *
  123  git commit "lab commit"
  124  git commit -m "lab commit"
  125  git push
  126  cd ..
  127  docker images
  128  docker lgin
  129  docker login
  130  vi /home/ec2-user/.docker/config.json
  131  docker build -t knowdocker/myapache:2.0 testapp/frontend/
  132  docker imges
  133  docker images
  134  docker push knowdocker/myapache:2.0
  135  cd testapp/
  136  git add *
  137  git commit -m "added first day notes"
  138  git push
  139  cd ..
  140  docker build -t knowdocker/nginx-php:1.0 testapp/excercise/
  141  docker run -dit ubuntu:18.04
  142  docker ps
  143  docker exec -it hopeful_blackwell /bin/bash
  144  docker run -dit ubuntu:18.04
  145  docker exec -it hopeful_blackwell /bin/bash
  146  docker build -t knowdocker/nginx-php:1.0 testapp/excercise/
  147  d
  148  docker images
  149  docker run -d --name nginx-apache -p 9001:80  knowdocker/nginx-php:1.0
  150  docker ps
  151  curl http://localhost:9001
  152  docker history knowdocker/nginx-php:1.0
  153  docker images
  154  docker rm 4188dd4b99bb
  155  docker image rmi 4188dd4b99bb
  156  docker image rmi -f 55607ad4ccca
  157  docker image rmi -f 4188dd4b99bb
  158  docker images
  159  docker rmi -f $(docker images -f "dangling=true" -q)
  160  docker images
  161  d
  162  cd testapp/
  163  git add *
  164  git commit -m "second docker file"
  165  git push
  166  cd 
  167  pwd
  168  exit
  169  docker login
  170  docker images
  171  docker push knowdocker/nginx-php:1.0
  172  curl http://localhost9001
  173  curl http://localhost:9001
  174  docker ps -a
  175  docker ps
  176  docker rm nginx-apache inspiring_shannon hopeful_blackwell
  177  docker stop nginx-apache inspiring_shannon hopeful_blackwell
  178  docker rm nginx-apache inspiring_shannon hopeful_blackwell
  179  docker images
  180  docker ps
  181  docker ps -a
  182  docker ps
  183  docker images
  184  docker run -d -p 9000:80 --name testapache -v testapache_var_log:/var/log -v /home/ec2-user:/home/ec2-user knowdocker/myapache:1.0
  185  docker run -d -p 9000:80 --name testapachevolume -v testapache_var_log:/var/log -v /home/ec2-user:/home/ec2-user knowdocker/myapache:1.0
  186  docker volumes
  187  docker volume
  188  docker images
  189  docker ps
  190  docker exec testapchevolme /bin/bash
  191  docker exec testapachevolme /bin/bash
  192  cd /home/
  193  ls
  194  cd ec2-user/
  195  ll
  196  ls
  197  docker volumes
  198  docker volume
  199  docker volume ls
  200  docker exec -it testapachevolume /bin/bash
  201  git clone https://githu.commohanraz81/awslogs.git
  202  git clone https://github.com/mohanraz81/awslogs.git
  203  ls
  204  mv ~/.aws ~/.aws_backup
  205  docker build -t mohanraz81/awslogs:1.0 /awslogs
  206  docker build -t mohanraz81/awslogs:1.0 awslogs/
  207  docker run -d --name logagent -v testapache_var_log
  208  docker run -d --name logagent -v testapache_var_log  mohanraz81/awslogs:1.0
  209  docker rm logagent
  210  docker stop logagent
  211  docker run -d --name logagent -v testapache_var_log:/mnt/log:ro  mohanraz81/awslogs:1.0 
  212  docker rm logagent
  213  docker run -d --name logagent -v testapache_var_log:/mnt/log:ro  mohanraz81/awslogs:1.0 
  214  docker exec -it logagent bash/bin
  215  curl http://localost:9000
  216  curl http://localhost:9000
  217  docker network ls
  218  docker network ps
  219  docker ps -a
  220  docker ps
  221  docker exec -it logagent /bin/sh
  222  docker stop logagent testapachevolume
  223  docker rm logagent testapachevolume
  224  docker ps
  225  docker ps -a
  226  docker network ls
  227  ifconfg -a|more
  228  ifconfig -a|more
  229  docker inspect bridge
  230  docker network create --drive=bridge --subnet 10.1.0.0/16 --gateway 10.1.0.1 testnw
  231  docker network create --driver=bridge --subnet 10.1.0.0/16 --gateway 10.1.0.1 testnw
  232  docker network ls
  233  docker history
  234  docker images
  235  docker run -d -p 9000:80 --name frontend --net testnw knowdocker/myapache:2.0
  236  docker run -d -p  --name backend --net testnw knowdocker/myapache:2.0
  237  docker run -d  --name backend --net testnw knowdocker/myapache:2.0
  238  docker ps
  239  docker inspect testnw
  240  curl http://frontend
  241  docker exec -it  frontend bash/bin
  242  docker exec -it  frontend bash\bin
  243  docker exec -it  frontend "bash\bin"
  244  docker exec -it  frontend "bin bash"
  245  docker exec -it  frontend bin bash
  246  docker stop frontend backend
  247  docker rm testnw
  248  docker --net rm testnw
  249  docker network rm testnw
  250  docker run -d -p  --name backend  knowdocker/myapache:2.0
  251  docker run -d  --name backend  knowdocker/myapache:2.0
  252  docker rm frontend backend
  253  docker run -d  --name backend  knowdocker/myapache:2.0
  254  docker run -d -p 9000:80 --name frontend --net testnw knowdocker/myapache:2.0
  255  docker run -d -p 9000:80 --name frontend  knowdocker/myapache:2.0
  256  docker rm fronend
  257  docker rm frontend
  258  docker run -d -p 9000:80 --name frontend  knowdocker/myapache:2.0 --link backend
  259  docker run -d -p 9000:80 --name frontend  knowdocker/myapache:2.0 --link backend knowdocker/myapache
  260  docker ps
  261  docker run -d -p 9000:80 --name frontend  knowdocker/myapache:2.0 --link backend knowdocker/myapache:2.0
  262  docker ps
  263  docker rm frontend
  264  docker run -d -p 9000:80 --name frontend  knowdocker/myapache:2.0 --link backend knowdocker/myapache:2.0
  265  docker run -d -p 9000:80 --name frontend  --link backend knowdocker/myapache:2.0
  266  docker rm frontend
  267  docker run -d -p 9000:80 --name frontend  --link backend knowdocker/myapache:2.0
  268  curl http://frontend
  269  curl http://frontend:9000
  270  docker -it exec frontend /bin/bash
  271  docker exec -it  frontend /bin/bash
  272  cd testapp
  273  ls
  274  cd testapp/
  275  git add *
  276  git commit -m "after network"
  277  git push
  278  cd ..
  279  history
  280  sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
  281  sudo chmod +x /usr/local/bin/docker-compose
  282  docker-compose
  283  docker images
  284  docker ps
  285  cd testapp/
  286  docker-compose build
  287  docker images
  288  docker-compose build
  289  docker images
  290  docker ps
  291  docker stop frontend backend
  292  docker rm frontend backend
  293  docker-compose up -d 
  294  docker ps
  295  docker-compose down
  296  git add *
  297  git commit -m "docker-compose"
  298  git push
  299  docker-compose down
  300  docker ps
  301  docker ps -a
  302  history
  303  docker-compose push
  304  history
  305  h
  306  cd testapp/
  307  git add *
  308  git commit -m "end of 2nd day commit"
  309  git push
  310  exit
  311  mv ~/.aws ~/.aws_backup
  312  pip install awscli --upgrade --user
  313  curl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.13.7/2019-06-11/bin/linux/amd64/aws-iam-authenticator
  314  chmod +x ./aws-iam-authenticator
  315  mkdir -p $HOME/bin && cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator && export PATH=$HOME/bin:$PATH
  316  echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
  317  aws-iam-authenticator help
  318  curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
  319  sudo mv /tmp/eksctl /usr/local/bin
  320  eksctl version
  321  eksctl create cluster --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 --node-ami auto
  322  $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
  323  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
  324  chmod +x ./kubectl
  325  sudo mv ./kubectl /usr/local/bin/kubectl
  326  kubectl version
  327  kubectl get nodes
  328  kubectl get prod
  329  kubectl get clusters
  330  kubectl get nodes
  331  kubectl get cluster
  332  kubectl get pods
  333  kubectl get nods
  334  kubectl get nodes
  335  kubectl cat ~/.kube/config
  336   cat ~/.kube/config
  337  kubectl pods
  338  kubectl get pods
  339  kubectl describe cluster prod
  340  kubectl describe node 
  341  kubectl describe node ip-192-168-0-101.us-west-2.compute.internal
  342  kubectl get namespaces
  343  kubectl get pods --namespace=kube-system
  344  kubectl get pods --namespace=kube-public
  345  kubectl get svc --namespace=kube-public
  346  kubectl create namespace test
  347  kubectl get namespace
  348  kubectl describe test
  349  kubectl describe namespace test
  350  kubectl config set-context $(kubectl config current-context) --namespace=test
  351  kubectl config view | grep namespace:test
  352  kubectl config view | grep namespace: test
  353  kubectl config view | grep namespace test
  354  kubectl config view | grep namespace
  355  kubectl config view
  356  kubectl config view | grep namespace
  357  kubectl apply -f testapp/deploy.yml
  358  kubectl get deployment
  359  kubectl describe deployment apache-deployment
  360  kubectl describe rs apache-deployment-b8bf884b
  361  kubectl events
  362  kubectl get service
  363  history
  364  eksctl delete cluster --region us-west-2 --name prod
  365  cd testapp/
  366  git add *
  367  git commit -m "day 3 end"
  368  git push
  369   mv ~/.aws ~/.aws_backup1
  370   eksctl version
  371  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 --node-ami auto
  372  --region us-east-1a --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 --node-ami aut
  373  eksctl create cluster --region us-east-1a --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 --node-ami auto
  374  vi home/ec2-user/.kube/config
  375  vi /home/ec2-user/.kube/config
  376  eksctl create cluster --region us-east-2 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.small --nodes 3 --nodes-min 3 --nodes-max 3 --node-ami auto
  377  kubectl get pods
  378  kubectl get nods
  379  kubectl get nodes
  380  kubectl get namespaces
  381  kubectl create namespace test
  382  kubectl get namespaces
  383  kubectl describe namespace test
  384  kubectl config set-context $(kubectl config current-context) --namespace=test
  385   kubectl apply -f testapp/deploy.yml
  386  get deployment
  387  kubectl get deployment
  388  describe deployment apache-deployment
  389  kubectl describe deployment apache-deployment
  390  kubectl describe rs apache-deployment-b8bf884b
  391  kubectl events
  392  kubectl get service
  393  eksctl delete cluster --region us-east-2 --name prod
  394   mv ~/.aws ~/.aws_backup4
  395   mv ~/.aws ~/.aws_backup
  396   mv ~/.aws ~/.aws_backup1
  397  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  398  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  399  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  400  kubectl  get namespaces
  401   kubectl pods
  402   kubectl get pods
  403   kubectl get nodes
  404  kubectl describe cluster prod
  405  kubectl describe node
  406  kubectl  get namespaces
  407  kubectl get nodes
  408  kubectl apply -f testapp/deploy.yml
  409  kubectl describe deployment apache-deployment
  410  kubectl describe srv testfrontend
  411  kubectl describe src testfrontend
  412  kubectl describe service testfrontend
  413  kubectl get pods
  414  kubectl describe pod apache-deployment-954897f69-bqnpb
  415  kubectl describe pod apache-deployment-954897f69-jcftb
  416  kubectl get hpa
  417  kubectl get hps
  418  kubectl get hpa
  419  curl -LO https://git.io/get_helm.sh
  420  chmod 700 get_helm.sh
  421  ./get_hemm.sh
  422  ./get_helm.sh
  423  kubectl create namespace tiller
  424  export TILLER_NAMESPACE = tiller
  425  export TILLER_NAMESPACE=tiller
  426  k
  427  export HELM_HOST=:44134
  428  helm init --client-only
  429  sudo yum -y install jq
  430  DOWNLOAD_URL=$(curl --silent "https://api.github.com/repos/kubernetes-incubator/metrics-server/releases/latest" | jq -r .tarball_url)
  431  DOWNLOAD_VERSION=$(grep -o '[^/v]*$' <<< $DOWNLOAD_URL)
  432  curl -Ls $DOWNLOAD_URL -o metrics-server-$DOWNLOAD_VERSION.tar.gz
  433  mkdir metrics-server-$DOWNLOAD_VERSION
  434  tar -xzf metrics-server-$DOWNLOAD_VERSION.tar.gz --directory metrics-server-$DOWNLOAD_VERSION --strip-components 1
  435  kubectl apply -f metrics-server-$DOWNLOAD_VERSION/deploy/1.8+
  436  kubectl get deployment metrics-server -n kube-system
  437  kubectl get --raw /metrics
  438  kubectl create namespace prometheus
  439  helm install stable/prometheus --name prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2",server.persistentVolume.storageClass="gp2"
  440  kubectl get pods -n prometheus
  441  kubectl get hpa
  442  kubectl get service
  443  kubectl get hpa
  444  kubectl get service
  445  kubectl get pods
  446  kubectl run -i --tty load-generator --image=busybox /bin/sh
  447  kubectl get deployment apache-deployment
  448  kubectl describe deployment apache-deployment
  449  kubectl get service
  450  kubectl describe service testfrontend
  451  kubectl get hpa
  452  kubectl get service
  453  kubectl describe service testfrontend
  454  kubectl attach load-generator-557649ddcd-xj55v -c load-generator -i -t
  455  kubectl delete all --all
  456  kubectl create namespace test
  457  kubectl config set-context $(kubectl config current-context) --namespace=test
  458   kubectl apply -f testapp/deploy.yml
  459  kubectl get pods
  460  kubectl get services
  461  docker build -t knowdocker/myfrontend:2.0 testapp/frontend/
  462  docker push knowdocker/myfrontend:2.0
  463  kubectl get rs
  464  kubectl apply -f testapp/deploy.yml
  465  docker build -t knowdocker/myfrontend:3.0 testapp/frontend/
  466  docker push knowdocker/myfrontend:3.0
  467  kubectl rollout history deployment.v1beta1.apps/nginx-deployment --revision=2
  468  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=2
  469  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=1
  470  kubectl rollout undo deployment.v1beta1.apps/apache-deployment
  471  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=2
  472  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=3
  473  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=2
  474  kubectl rollout history deployment.v1beta1.apps/apache-deployment --revision=1
  475  kubectl rollout undo deployment.v1beta1.apps/apache-deployment --to-revision=1
  476  kubectl rollout history deployment.v1beta1.apps/apache-deployment
  477  kubectl rollout history deployment.v1beta1.apps/nginx-deployment
  478  kubectl rollout history deployment.v1beta1.apps/apache-deployment deployments "apache-deployment"
  479  kubectl rollout history deployment.v1beta1.apps/apache-deployment
  480  kubectl delete all --all
  481  kubectl apply -f testapp/two-tier/deploy.yml
  482  kubectl service hello
  483  kubectl deployment hello
  484  kubectl get service
  485  kubectl create namespace newtest
  486  kubectl apply -f testapp/two-tier/deploy.yml --namespace=newtest
  487  kubectl get pods
  488  kubectl get pods --namespace=newtest
  489  kubectl apply -f testapp/two-tier/deploy.yml --namespace=newtestone
  490  kubectl exec -it kubectl frontend-c8665bf8b-cptf6
  491  kubectl exec -it frontend-c8665bf8b-cptf6
  492  kubectl exec -it frontend-c8665bf8b-cptf6 bin/bash
  493  kubectl exec -it --namespact=newtest  frontend-c8665bf8b-cptf6 bin/bash
  494  history
  495  kubectl --namespace=newtest exec -it frontend-c8665bf8b-cptf6 /bin/bash
  496  kubectl delete all --all --namesace=newtest
  497  kubectl delete all --all --namespace=newtest
  498  kubectl delete all --all --namespace=test
  499  kucbectl get namespace
  500  kubectl get namespace
  501  kubectl delete all --all --namespace=test
  502  kubectl delete all --all --namespace=newtest
  503  kubectl get namespaces
  504  kubectl delete namepace test
  505  kubectl delete namespace test
  506  kubectl delete namespace newtest
  507  kubectl apply -f testapp/test-quota.yml
  508  kubectl create namespace test
  509  kubectl config set-context $(kubectl config current-context) --namespace=test
  510  kubectl apply -f testapp/test-quota.yml
  511  kubectl get service
  512  kubectl describe namespace test
  513  kubectl apply -f testapp/test-limitrange.yml
  514  kubectl describe namespace test
  515  kubectl delete all --all namespace test
  516  kubectl delete all --all namespace=test
  517  kubectl delete all --all --namespace=test
  518  kubectl delete all --all
  519  kubectl get namespaces
  520  kubectl delete all --all
  521  kubectl delete all --all
  522  kubectl describe namespace testkubectl describe namespace test
  523  kubectl get namespaces
  524  kubectl delete namespace=test
  525  kubectl delete namespace=test --all
  526  kubectl delete namespace
  527  kubectl delete all --all
  528  kubectl delete namespace test
  529  kubectl delete all --all
  530  kubectl get pods
  531  kubectl get service
  532  history
  533  git add *
  534  cd testapp/
  535  git add *
  536  git commit -m "end of day4"
  537  git push
  538  tiller -listen=localhost:44134 -storage=secret -logtostderr
  539  eksctl delete cluster --region us-east-1 --name prod
  540  exit
  541  cd testapp/
  542  git add *
  543  git commit -m "Day 4 EOD second commit"
  544  git push
  545  exit
  546  history
  547  cd testapp/
  548  git add *
  549  git commit -m "day 4 history added comment"
  550  git push
  551  exit
  552  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  553  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  554  eksctl create cluster --region us-east-1 --name prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto
  555  eksctl create cluster --region us-east-1 --name prod_new --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto--appmesh-access
  556   eksctl create cluster --region us-east-1 --name prod_new --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto --appmesh-access
  557  eksctl create cluster --region us-east-1 --name new_prod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto --appmesh-access
  558  eksctl create cluster --region us-east-1 --name newprod --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto --appmesh-access
  559  git clone https://github.com/aws/aws-app-mesh-examples
  560  cd aws-app-mesh-examples/examples/apps/djapp/
  561  eksctl delete cluster --region us-east-1 --name newprod
  562  eksctl create cluster --region us-east-1 --name prodnew --version 1.13 --nodegroup-name standard-workers --node-type t3.medium --zones=us-east-1a,us-east-1b,us-east-1c --nodes 4 --nodes-min 4 --nodes-max 4 --node-ami auto --appmesh-access
  563  kubectl get nodes
  564  ll
  565  cd ..
  566  ll
  567  cd aws-app-mesh-examples/examples/apps/djapp/
  568  cd ..
  569  ll
  570  cd apps
  571  cd djapp
  572  ll
  573  echo $ROLE_NAME
  574  echo $AWS_REGION
  575  export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)
  576  export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
  577  echo "export ACCOUNT_ID=${ACCOUNT_ID}" >> ~/.bash_profile
  578  echo "export AWS_REGION=${AWS_REGION}" >> ~/.bash_profile
  579  aws configure set default.region ${AWS_REGION}
  580  aws configure get default.region
  581  aws iam get-role-policy --role-name $ROLE_NAME --policy-name AM-Policy-For-Worker
  582  ll
  583  kubectl apply -f 1_create_the_initial_architecture/1_prod_ns.yaml
  584  kubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_deployment.yaml
  585  kubectl apply -nprod -f 1_create_the_initial_architecture/1_initial_architecture_services.yaml
  586  kubectl get all -nprod
  587  kubectl get pods -nprod -l app=dj
  588  kubectl exec -nprod -it dj-7fbb4d499c-gk5dn bash
  589  cd 2_create_injector
  590  ll
  591  ./create.sh
  592  error
  593   cd ..
  594  kubectl label namespace prod appmesh.k8s.aws/sidecarInjectorWebhook=enabled
  595  kubectl get pods -nappmesh-inject
  596  kubectl get pods -nprod
  597  kubectl get pods -nappmesh-inject
  598  kubectl apply -f 3_add_crds/mesh-definition.yaml
  599  kubectl apply -f 3_add_crds/virtual-node-definition.yaml
  600  kubectl apply -f 3_add_crds/virtual-service-definition.yaml
  601  kubectl apply -f 3_add_crds/controller-deployment.yaml
  602  kubectl get pods -nappmesh-system
  603  kubectl create -f 4_create_initial_mesh_components/mesh.yaml
  604  kubectl get meshes -nprod
  605  aws appmesh list-meshes
  606  aws appmesh describe-mesh --mesh-name dj-app
  607  kubectl create -f 4_create_initial_mesh_components/nodes_representing_virtual_services.yaml
  608  kubectl create -nprod -f 4_create_initial_mesh_components/nodes_representing_physical_services.yaml
  609  kubectl get virtualnodes -nprod
  610  kubectl apply -nprod -f 4_create_initial_mesh_components/virtual-services.yaml
  611  kubectl get servcie
  612  kubectl get service
  613  kubectl get services
  614  kubectl get svc -nprod
  615  kubectl create -nprod -f 4_create_initial_mesh_components/metal_and_jazz_placeholder_services.yaml
  616  kubectl get -nprod virtualservices
  617  kubectl get svc -nprod
  618  kubectl get pods -nprod
  619  kubectl describe pods/dj-5b445fbdf4-qf8sv -nprod
  620  kubectl describe pods/dj-7fbb4d499c-gk5dn -nprod
  621  kubectl get pods -nprod
  622  kubectl patch deployment dj -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
  623  kubectl patch deployment metal-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
  624  kubectl patch deployment jazz-v1 -nprod -p "{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"`date +'%s'`\"}}}}}"
  625  kubectl get pods -nprod
  626  kubectl describe pods/dj-5b445fbdf4-qf8sv -nprod
  627  kubectl describe pods/dj-7fbb4d499c-gk5dn -nprod
  628  kubectl get pods -nprod
  629  kubectl describe pods/dj-b59c96cc5-zqwgc -nprod
  630  kubectl get pods -nprod -lapp=dj
  631  kubectl exec -nprod -it dj-b59c96cc5-zqwgc -c dj bash
  632  kubectl apply -nprod -f 5_canary/jazz_v2.yaml
  633  kubectl describe virtualservice jazz -nprod
  634  kubectl apply -nprod -f 5_canary/jazz_service_update.yaml
  635  kubectl describe virtualservice jazz -nprod
  636  kubectl apply -nprod -f 5_canary/metal_v2.yaml
  637  kubectl apply -nprod -f 5_canary/metal_service_update.yaml
  638  kubectl describe virtualservice metal -nprod
  639  kubectl get pods -nprod -l app=dj
  640  kubectl exec -nprod -it dj-b59c96cc5-zqwgc -c dj bash
  641  ./cleanup.sh
  642  exit
  643  cd testapp/
  644  kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
  645  kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
  646  kubectl apply -f testapp/sql/mysql-statefulset.yaml
  647  kubectl getpods
  648  kubectl get pods
  649  kubectl get pvc
  650  kubectl get pods -l app=mysql --watch
  651  kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --  mysql -h mysql-0.mysql <<EOFCREATE DATABASE test;
  652  CREATE TABLE test.messages (message VARCHAR(250));
  653  INSERT INTO test.messages VALUES ('hello');
  654  EOF
  655  kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --  mysql -h mysql-read -e "SELECT * FROM test.messages"
  656  kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --  bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"
  657  kubectl create namespace tiller
  658  kubectl config set-context $(kubectl config current-context) --namespace=tiller
  659  export TILLER_NAMESPACE=tiller
  660  export HELM_HOST=:44134
  661  helm init --client-only
  662  helm repo update
  663  helm create testappchart
  664  cd testapp
  665  helm create testappchart
  666  helm verify
  667  helm lint testappchart
  668  cd cd testappchart/
  669  cd testappchart/
  670  helm lint testappchart
  671  cd testapp
  672  cd testappchart/
  673  helm lint testappchart
  674  helm version
  675  tiller -listen=localhost:44134 -storage=secret -logtostderr
  676  cd testappchart/
  677   export TILLER_NAMESPACE=tiller
  678  export HELM_HOST=:44134
  679  helm verify
  680  helm lint testappchart
  681  helm version
  682  helm lint
  683  helm lint testappchart
  684  cd ..
  685  helm lint testappchart
  686  git clone https://github.com/rsubramanian4/threetier.git
  687  tiller -listen=localhost:44134 -storage=secret -logtostderr
  688  helm package 
  689  exit
  690  cd threetier/
  691  kubectl create namespace test
  692   kubectl config set-context $(kubectl config current-context) --namespace=test
  693  sudo yum -y install java-1.8.0-openjdk.x86_64
  694  sudo yum -y remove java-1.7.0-openjdk
  695  sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo
  696  $sudo  rpm --import http://pkg.jenkins-ci.org/redhat-stable/jenkins-ci.org.key
  697   sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo
  698  sudo  rpm --import http://pkg.jenkins-ci.org/redhat-stable/jenkins-ci.org.key
  699  sudo yum install -y  jenkins
  700  sudo systemctl start jenkins
  701  sudo start jenkins
  702  sudo systemctl start jenkins
  703  sudo service jenkins start
  704  cat /var/lib/jenkins/secrets/initialAdminPassword
  705  sudo cat /var/lib/jenkins/secrets/initialAdminPassword
  706   delete cluster --region us-east-1 --name prodnew
  707   eksctl delete cluster --region us-east-1 --name prodnew
  708  git add *
  709  git commit -m "three tier"
  710  git push
  711  cd ..
  712  cd testapp
  713  git add *
  714  git commit -m "finalday"
  715  git pusk
  716  git push
  717  history
AP-ROLE-NAME6:~/environment/testapp (master) $ 



